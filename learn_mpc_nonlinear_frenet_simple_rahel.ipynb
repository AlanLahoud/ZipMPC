{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Function, Variable\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils import parameters_to_vector\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Comment these lines if your MPC is in the current directory.\n",
    "# Otherwise modify to the directory.\n",
    "#import sys\n",
    "#sys.path.append('./../mpc.pytorch-master/')\n",
    "\n",
    "\n",
    "from mpc import mpc\n",
    "from mpc import casadi_control\n",
    "from mpc.mpc import GradMethods, QuadCost, LinDx\n",
    "#from mpc.dynamics import NNDynamics\n",
    "#import mpc.util as eutil\n",
    "from mpc.env_dx import frenet_kin_bicycle\n",
    "from mpc.track.src import simple_track_generator, track_functions\n",
    "\n",
    "\n",
    "\n",
    "#import sys\n",
    "#from IPython.core import ultratb\n",
    "#sys.excepthook = ultratb.FormattedTB(mode='Verbose',\n",
    "#     color_scheme='Linux', call_pdb=1)\n",
    "\n",
    "import time\n",
    "import os\n",
    "import shutil\n",
    "import pickle as pkl\n",
    "import collections\n",
    "\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try to create a track \n",
    "\n",
    "track_density = 300\n",
    "track_width = 0.5\n",
    "v_max = 2\n",
    "gen = simple_track_generator.trackGenerator(track_density,track_width)\n",
    "track_name = 'DEMO_TRACK'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 0.3\n",
    "init = [0,0,0]\n",
    "\n",
    "track_function = {\n",
    "    'DEMO_TRACK'    : track_functions.demo_track,\n",
    "    'HARD_TRACK'    : track_functions.hard_track,\n",
    "    'LONG_TRACK'    : track_functions.long_track,\n",
    "    'LUCERNE_TRACK' : track_functions.lucerne_track,\n",
    "    'BERN_TRACK'    : track_functions.bern_track,\n",
    "    'INFINITY_TRACK': track_functions.infinity_track,\n",
    "    'SNAIL_TRACK'   : track_functions.snail_track\n",
    "}.get(track_name, track_functions.demo_track)\n",
    "    \n",
    "track_function(gen, t, init)\n",
    "    \n",
    "gen.populatePointsAndArcLength()\n",
    "gen.centerTrack()\n",
    "\n",
    "track_coord = torch.from_numpy(np.vstack([gen.xCoords, gen.yCoords, gen.arcLength, gen.tangentAngle, gen.curvature]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "softplus_op = torch.nn.Softplus(10)\n",
    "\n",
    "def sample_xinit(n_batch):\n",
    "    def uniform(shape, low, high):\n",
    "        r = high-low\n",
    "        return torch.rand(shape)*r+low\n",
    "\n",
    "    sigma = uniform(n_batch, 0.01, 2.)\n",
    "    d = uniform(n_batch, -0.1, 0.1)\n",
    "    phi = uniform(n_batch, -0.40*np.pi, 0.40*np.pi)\n",
    "    v = uniform(n_batch, 0., 0.2)\n",
    "    sigma_0 = sigma\n",
    "    sigma_diff = sigma-sigma_0\n",
    "    d_lb = softplus_op(-d-0.5*track_width)\n",
    "    d_ub = softplus_op(d-0.5*track_width)\n",
    "    v_lb = softplus_op(-v + 0)\n",
    "    v_ub = softplus_op(v-v_max)\n",
    "    xinit = torch.stack((sigma, d, phi, v, sigma_0, sigma_diff, d_lb, d_ub, v_lb, v_ub), dim=1)\n",
    "\n",
    "    return xinit\n",
    "\n",
    "true_dx = frenet_kin_bicycle.FrenetKinBicycleDx(track_coord)\n",
    "mpc_T = 15\n",
    "n_batch = 8\n",
    "\n",
    "# Added here the bounds of U\n",
    "u_lower = torch.tensor([-2., -1.]).unsqueeze(0).unsqueeze(0).repeat(mpc_T, n_batch, 1)\n",
    "u_upper = torch.tensor([2., 1.]).unsqueeze(0).unsqueeze(0).repeat(mpc_T, n_batch, 1)\n",
    "\n",
    "n_state = true_dx.n_state\n",
    "print(n_state)\n",
    "n_ctrl = true_dx.n_ctrl\n",
    "\n",
    "u_init=None\n",
    "eps = 1\n",
    "lqr_iter = 500\n",
    "grad_method = GradMethods.AUTO_DIFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6.6548526e-01  4.7545321e-02 -9.6970081e-02  1.9080684e-01\n",
      "  6.6548526e-01  0.0000000e+00  4.9765217e-03  1.2403339e-02\n",
      "  1.3834067e-02  1.3892311e-09]\n",
      "curv start\n",
      "curv end\n",
      "curv start\n",
      "curv end\n",
      "curv start\n",
      "curv end\n",
      "solve optimization problem\n",
      "\n",
      "******************************************************************************\n",
      "This program contains Ipopt, a library for large-scale nonlinear optimization.\n",
      " Ipopt is released as open source code under the Eclipse Public License (EPL).\n",
      "         For more information visit https://github.com/coin-or/Ipopt\n",
      "******************************************************************************\n",
      "\n",
      "This is Ipopt version 3.14.11, running with linear solver MUMPS 5.4.1.\n",
      "\n",
      "Number of nonzeros in equality constraint Jacobian...:      304\n",
      "Number of nonzeros in inequality constraint Jacobian.:       30\n",
      "Number of nonzeros in Lagrangian Hessian.............:      240\n",
      "\n",
      "Total number of variables............................:       94\n",
      "                     variables with only lower bounds:        0\n",
      "                variables with lower and upper bounds:        0\n",
      "                     variables with only upper bounds:        0\n",
      "Total number of equality constraints.................:       64\n",
      "Total number of inequality constraints...............:       30\n",
      "        inequality constraints with only lower bounds:        0\n",
      "   inequality constraints with lower and upper bounds:       30\n",
      "        inequality constraints with only upper bounds:        0\n",
      "\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "   0  1.9964558e+01 6.65e-01 1.64e+01  -1.0 0.00e+00    -  0.00e+00 0.00e+00   0\n",
      "   1  1.8332930e+01 5.57e-01 5.00e+01  -1.0 1.22e+01    -  1.41e-01 1.63e-01f  1\n",
      "   2 -3.2713157e+00 3.56e-02 2.34e+00  -1.0 6.13e-01    -  4.22e-01 1.00e+00f  1\n",
      "   3 -6.4552420e+00 4.06e-03 3.10e-01  -1.0 1.53e+00    -  5.37e-01 1.00e+00f  1\n",
      "   4 -6.5670093e+00 1.10e-05 3.14e-04  -1.7 2.98e-01    -  1.00e+00 1.00e+00f  1\n",
      "   5 -6.5686464e+00 3.81e-07 2.04e-06  -2.5 3.69e-02    -  1.00e+00 1.00e+00h  1\n",
      "   6 -6.5686540e+00 6.67e-09 3.41e-08  -3.8 1.66e-03    -  1.00e+00 1.00e+00h  1\n",
      "   7 -6.5686541e+00 1.35e-11 5.82e-11  -5.7 3.25e-05    -  1.00e+00 1.00e+00h  1\n",
      "   8 -6.5686541e+00 1.98e-15 7.99e-15  -8.6 3.68e-07    -  1.00e+00 1.00e+00h  1\n",
      "\n",
      "Number of Iterations....: 8\n",
      "\n",
      "                                   (scaled)                 (unscaled)\n",
      "Objective...............:  -6.5686540712337624e+00   -6.5686540712337624e+00\n",
      "Dual infeasibility......:   7.9936057773011271e-15    7.9936057773011271e-15\n",
      "Constraint violation....:   1.9845236565174673e-15    1.9845236565174673e-15\n",
      "Variable bound violation:   0.0000000000000000e+00    0.0000000000000000e+00\n",
      "Complementarity.........:   2.5064275574384279e-09    2.5064275574384279e-09\n",
      "Overall NLP error.......:   2.5064275574384279e-09    2.5064275574384279e-09\n",
      "\n",
      "\n",
      "Number of objective function evaluations             = 9\n",
      "Number of objective gradient evaluations             = 9\n",
      "Number of equality constraint evaluations            = 9\n",
      "Number of inequality constraint evaluations          = 9\n",
      "Number of equality constraint Jacobian evaluations   = 9\n",
      "Number of inequality constraint Jacobian evaluations = 9\n",
      "Number of Lagrangian Hessian evaluations             = 8\n",
      "Total seconds in IPOPT                               = 0.013\n",
      "\n",
      "EXIT: Optimal Solution Found.\n",
      "      solver  :   t_proc      (avg)   t_wall      (avg)    n_eval\n",
      "       nlp_f  |  18.00us (  2.00us)  16.86us (  1.87us)         9\n",
      "       nlp_g  | 212.00us ( 23.56us) 204.89us ( 22.77us)         9\n",
      "  nlp_grad_f  |  29.00us (  2.90us)  28.59us (  2.86us)        10\n",
      "  nlp_hess_l  | 755.00us ( 94.38us) 756.74us ( 94.59us)         8\n",
      "   nlp_jac_g  | 324.00us ( 32.40us) 325.18us ( 32.52us)        10\n",
      "       total  |   6.95ms (  6.95ms)  15.69ms ( 15.69ms)         1\n",
      "[[ 7.04113298e-01  2.14370178e-03]\n",
      " [ 6.74047358e-01  8.78982625e-03]\n",
      " [ 6.46389139e-01  1.45652969e-02]\n",
      " [ 6.21148354e-01  1.94760113e-02]\n",
      " [ 5.98331494e-01  2.35107500e-02]\n",
      " [ 5.77941231e-01  2.66418967e-02]\n",
      " [ 5.59975500e-01  2.88252057e-02]\n",
      " [ 5.44426158e-01  2.99987597e-02]\n",
      " [ 5.31277110e-01  3.00812299e-02]\n",
      " [ 5.20501711e-01  2.89695445e-02]\n",
      " [ 5.12059228e-01  2.65360847e-02]\n",
      " [ 5.05890096e-01  2.26255733e-02]\n",
      " [ 5.01909642e-01  1.70519022e-02]\n",
      " [ 5.00000000e-01  9.59525644e-03]\n",
      " [ 5.00000000e-01 -1.88994216e-20]]\n",
      "[[ 0.66548526  0.04754532 -0.09697008  0.19080684]\n",
      " [ 0.67515636  0.04667524 -0.10059096  0.2260125 ]\n",
      " [ 0.68661641  0.04576016 -0.10487772  0.25971487]\n",
      " [ 0.69979082  0.04476489 -0.1098782   0.29203433]\n",
      " [ 0.7146107   0.04365066 -0.11567466  0.32309174]\n",
      " [ 0.73101247  0.04237399 -0.1223843   0.35300832]\n",
      " [ 0.74893735  0.04088541 -0.13016045  0.38190538]\n",
      " [ 0.76833046  0.03912776 -0.1391941   0.40990415]\n",
      " [ 0.78913965  0.03703429 -0.14971542  0.43712546]\n",
      " [ 0.81131366  0.03452648 -0.16199473  0.46368932]\n",
      " [ 0.83479971  0.03151151 -0.1763422   0.4897144 ]\n",
      " [ 0.85954001  0.02787951 -0.19310516  0.51531736]\n",
      " [ 0.88546715  0.02350056 -0.21266167  0.54061187]\n",
      " [ 0.91249815  0.01822175 -0.23540841  0.56570735]\n",
      " [ 0.94052691  0.01186435 -0.26174138  0.59070735]\n",
      " [ 0.96941566  0.00422169 -0.29202794  0.61570735]]\n"
     ]
    }
   ],
   "source": [
    "# casadi mpc with exact penalty\n",
    "test_q = np.array([ 0.,  6.,  1.,  0., 0., 0., 0., 0., 1., 2.])\n",
    "test_p = np.array([ -2.,  0.,  0.,  0., 100., 100., 100., 100., -1,  0.])\n",
    "\n",
    "control = casadi_control.CasadiControl(track_coord)\n",
    "\n",
    "x0 = (sample_xinit(1)).numpy()\n",
    "print(x0[0])\n",
    "dc = 4 #number constraints\n",
    "df = 2 #number of states we do not really need with casadi, like simga_0 and sigma_diff\n",
    "dx = n_state #number states\n",
    "du = n_ctrl #number control inputs\n",
    "horizon = mpc_T\n",
    "sol = control.mpc_casadi(test_q,test_p,x0,horizon,df,dc,dx,du,track_width,v_max)\n",
    "N = horizon\n",
    "u = sol[-du*N:]\n",
    "x = sol[:-du*N]\n",
    "u_r = u.reshape(N,du)\n",
    "x_r = x.reshape(N+1,dx-df-dc)\n",
    "print(u_r)\n",
    "print(x_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curv start\n",
      "curv end\n",
      "curv start\n",
      "curv end\n",
      "curv start\n",
      "curv end\n",
      "This is Ipopt version 3.14.11, running with linear solver MUMPS 5.4.1.\n",
      "\n",
      "Number of nonzeros in equality constraint Jacobian...:      304\n",
      "Number of nonzeros in inequality constraint Jacobian.:       62\n",
      "Number of nonzeros in Lagrangian Hessian.............:      225\n",
      "\n",
      "Total number of variables............................:       94\n",
      "                     variables with only lower bounds:        0\n",
      "                variables with lower and upper bounds:        0\n",
      "                     variables with only upper bounds:        0\n",
      "Total number of equality constraints.................:       64\n",
      "Total number of inequality constraints...............:       62\n",
      "        inequality constraints with only lower bounds:        0\n",
      "   inequality constraints with lower and upper bounds:       62\n",
      "        inequality constraints with only upper bounds:        0\n",
      "\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "   0  1.9964558e+01 6.65e-01 5.72e-01  -1.0 0.00e+00    -  0.00e+00 0.00e+00   0\n",
      "   1 -3.0571804e-01 4.47e-03 7.33e+00  -1.0 7.74e-01    -  1.09e-01 1.00e+00f  1\n",
      "   2 -5.6719763e+00 3.83e-03 5.38e-01  -1.0 6.90e-01    -  6.06e-01 1.00e+00f  1\n",
      "   3 -6.5366938e+00 9.51e-04 3.72e-02  -1.7 3.02e-01    -  9.19e-01 1.00e+00f  1\n",
      "   4 -6.5683964e+00 4.73e-05 2.21e-04  -2.5 6.10e-02    -  1.00e+00 1.00e+00h  1\n",
      "   5 -6.5686536e+00 8.60e-08 2.57e-07  -3.8 2.56e-03    -  1.00e+00 1.00e+00h  1\n",
      "   6 -6.5686541e+00 9.95e-12 1.58e-10  -5.7 6.21e-05    -  1.00e+00 1.00e+00h  1\n",
      "   7 -6.5686541e+00 2.47e-15 3.69e-14  -8.6 9.30e-07    -  1.00e+00 1.00e+00h  1\n",
      "\n",
      "Number of Iterations....: 7\n",
      "\n",
      "                                   (scaled)                 (unscaled)\n",
      "Objective...............:  -6.5686540712338060e+00   -6.5686540712338060e+00\n",
      "Dual infeasibility......:   3.6859404417555197e-14    3.6859404417555197e-14\n",
      "Constraint violation....:   2.4702462297909733e-15    2.4702462297909733e-15\n",
      "Variable bound violation:   0.0000000000000000e+00    0.0000000000000000e+00\n",
      "Complementarity.........:   2.5067811794585629e-09    2.5067811794585629e-09\n",
      "Overall NLP error.......:   2.5067811794585629e-09    2.5067811794585629e-09\n",
      "\n",
      "\n",
      "Number of objective function evaluations             = 8\n",
      "Number of objective gradient evaluations             = 8\n",
      "Number of equality constraint evaluations            = 8\n",
      "Number of inequality constraint evaluations          = 8\n",
      "Number of equality constraint Jacobian evaluations   = 8\n",
      "Number of inequality constraint Jacobian evaluations = 8\n",
      "Number of Lagrangian Hessian evaluations             = 7\n",
      "Total seconds in IPOPT                               = 0.006\n",
      "\n",
      "EXIT: Optimal Solution Found.\n",
      "      solver  :   t_proc      (avg)   t_wall      (avg)    n_eval\n",
      "       nlp_f  |  12.00us (  1.50us)  10.88us (  1.36us)         8\n",
      "       nlp_g  | 208.00us ( 26.00us) 202.22us ( 25.28us)         8\n",
      "  nlp_grad_f  |  15.00us (  1.67us)  17.10us (  1.90us)         9\n",
      "  nlp_hess_l  | 696.00us ( 99.43us) 696.83us ( 99.55us)         7\n",
      "   nlp_jac_g  | 319.00us ( 35.44us) 318.64us ( 35.40us)         9\n",
      "       total  |   6.57ms (  6.57ms)   6.59ms (  6.59ms)         1\n",
      "[[ 7.04113300e-01  2.14370174e-03]\n",
      " [ 6.74047360e-01  8.78982624e-03]\n",
      " [ 6.46389141e-01  1.45652969e-02]\n",
      " [ 6.21148355e-01  1.94760113e-02]\n",
      " [ 5.98331495e-01  2.35107500e-02]\n",
      " [ 5.77941232e-01  2.66418967e-02]\n",
      " [ 5.59975500e-01  2.88252058e-02]\n",
      " [ 5.44426159e-01  2.99987598e-02]\n",
      " [ 5.31277111e-01  3.00812299e-02]\n",
      " [ 5.20501711e-01  2.89695445e-02]\n",
      " [ 5.12059228e-01  2.65360847e-02]\n",
      " [ 5.05890096e-01  2.26255733e-02]\n",
      " [ 5.01909642e-01  1.70519023e-02]\n",
      " [ 5.00000000e-01  9.59525646e-03]\n",
      " [ 5.00000000e-01 -5.32595335e-13]]\n",
      "[[ 0.66548526  0.04754532 -0.09697008  0.19080684]\n",
      " [ 0.67515636  0.04667524 -0.10059096  0.2260125 ]\n",
      " [ 0.68661641  0.04576016 -0.10487772  0.25971487]\n",
      " [ 0.69979082  0.04476489 -0.1098782   0.29203433]\n",
      " [ 0.7146107   0.04365066 -0.11567466  0.32309174]\n",
      " [ 0.73101247  0.04237399 -0.1223843   0.35300832]\n",
      " [ 0.74893735  0.04088541 -0.13016045  0.38190538]\n",
      " [ 0.76833046  0.03912776 -0.1391941   0.40990415]\n",
      " [ 0.78913965  0.03703429 -0.14971542  0.43712546]\n",
      " [ 0.81131366  0.03452648 -0.16199473  0.46368932]\n",
      " [ 0.83479971  0.03151151 -0.1763422   0.4897144 ]\n",
      " [ 0.85954001  0.02787951 -0.19310516  0.51531737]\n",
      " [ 0.88546715  0.02350056 -0.21266167  0.54061187]\n",
      " [ 0.91249815  0.01822175 -0.23540841  0.56570735]\n",
      " [ 0.94052691  0.01186435 -0.26174138  0.59070735]\n",
      " [ 0.96941566  0.00422169 -0.29202794  0.61570735]]\n"
     ]
    }
   ],
   "source": [
    "#casadi mpc with state constraints - NOTE: cost parameter vector has now different dimensions\n",
    "test_q = np.array([ 0.,  6.,  1.,  0., 1., 2.])\n",
    "test_p = np.array([ -2.,  0.,  0.,  0.,  -1,  0.])\n",
    "\n",
    "sol = control.mpc_casadi_with_constraints(test_q,test_p,x0,horizon,df,dc,dx,du,track_width,v_max)\n",
    "N = horizon\n",
    "u = sol[-du*N:]\n",
    "x = sol[:-du*N]\n",
    "u_r = u.reshape(N,du)\n",
    "x_r = x.reshape(N+1,dx-df-dc)\n",
    "print(u_r)\n",
    "print(x_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each mpc returns a prediction of the states and the inputs over the horizon for all batches and all dimensions,\n",
    "# they are stored in the following order pred_x = [horizon,batch, dimension].\n",
    "# To penalize a control behaviour that does not enforce enough progress within a horizon we can try to chose \n",
    "# our loss as -pred_x[mpc_T-1,:,0]\n",
    "\n",
    "def get_loss_progress(x_init, dx, _Q, _p, mpc_T=mpc_T):    \n",
    "        \n",
    "        pred_x, pred_u, pred_objs = mpc.MPC(\n",
    "            dx.n_state, dx.n_ctrl, mpc_T,\n",
    "            u_lower=u_lower, u_upper=u_upper, u_init=u_init,\n",
    "            lqr_iter=lqr_iter,\n",
    "            verbose=0,\n",
    "            exit_unconverged=False,\n",
    "            detach_unconverged=True,\n",
    "            linesearch_decay=dx.linesearch_decay,\n",
    "            max_linesearch_iter=dx.max_linesearch_iter,\n",
    "            grad_method=grad_method,\n",
    "            eps=5,\n",
    "            n_batch=n_batch,\n",
    "        )(x_init, QuadCost(_Q, _p), dx)\n",
    "        \n",
    "        #I added the second term to account the initial state (to kind of normalize the progress)\n",
    "        progress_loss = torch.mean(-pred_x[mpc_T-1,:,0] + pred_x[0,:,0])\n",
    "        \n",
    "        #penalty_loss = pred_x[]\n",
    "            \n",
    "        return progress_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "params1 = []\n",
    "learn_q_logit_state = torch.ones(n_state-7, requires_grad=True).to(device)\n",
    "learn_q_logit_sigma_diff = torch.ones(1, requires_grad=True).to(device)\n",
    "learn_q_logit_input = torch.ones(n_ctrl, requires_grad=True).to(device)\n",
    "learn_p_state = torch.zeros(n_state-7, requires_grad=True).to(device)\n",
    "learn_p_sigma_diff = torch.zeros(1, requires_grad=True).to(device)\n",
    "learn_p_input = torch.zeros(n_ctrl, requires_grad=True).to(device)\n",
    "params1 += [learn_q_logit_state, learn_q_logit_sigma_diff, learn_q_logit_input, learn_p_state, learn_p_sigma_diff, learn_p_input]\n",
    "env_params = true_dx.params\n",
    "q_penalty = .00*torch.ones(4).to(device)\n",
    "p_penalty = 10.0*torch.ones(4).to(device)\n",
    "q_sigma = .00*torch.ones(1).to(device)\n",
    "p_sigma = .00*torch.ones(1).to(device)\n",
    "q_sigma_0 = .00*torch.ones(1).to(device)\n",
    "p_sigma_0 = .00*torch.ones(1).to(device)\n",
    "track_coord = track_coord.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn_q_logit = torch.ones_like(true_q, requires_grad=True).to(device)\n",
    "#learn_p = torch.zeros_like(true_p, requires_grad=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "params = [{\n",
    "            'params': params1,\n",
    "            'lr': 2e-3,\n",
    "            'alpha': 0.99,\n",
    "            }]\n",
    "dx = true_dx.__class__(track_coord,env_params)\n",
    "\n",
    "opt = optim.RMSprop(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1.],\n",
      "       grad_fn=<CatBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alan/Desktop/envs/pao_env/lib/python3.8/site-packages/torch/autograd/__init__.py:411: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11060). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "/home/alan/Desktop/envs/pao_env/lib/python3.8/site-packages/torch/_tensor.py:770: UserWarning: torch.lu is deprecated in favor of torch.linalg.lu_factor / torch.linalg.lu_factor_ex and will be removed in a future PyTorch release.\n",
      "LU, pivots = torch.lu(A, compute_pivots)\n",
      "should be replaced with\n",
      "LU, pivots = torch.linalg.lu_factor(A, compute_pivots)\n",
      "and\n",
      "LU, pivots, info = torch.lu(A, compute_pivots, get_infos=True)\n",
      "should be replaced with\n",
      "LU, pivots, info = torch.linalg.lu_factor_ex(A, compute_pivots) (Triggered internally at ../aten/src/ATen/native/BatchLinearAlgebra.cpp:1992.)\n",
      "  LU, pivots, infos = torch._lu_with_info(\n",
      "/home/alan/Desktop/Research/Code/race_application/mpc/pnqp.py:19: UserWarning: torch.lu_solve is deprecated in favor of torch.linalg.lu_solveand will be removed in a future PyTorch release.\n",
      "Note that torch.linalg.lu_solve has its arguments reversed.\n",
      "X = torch.lu_solve(B, LU, pivots)\n",
      "should be replaced with\n",
      "X = torch.linalg.lu_solve(LU, pivots, B) (Triggered internally at ../aten/src/ATen/native/BatchLinearAlgebra.cpp:2149.)\n",
      "  x_init = -q.unsqueeze(2).lu_solve(*H_lu).squeeze(2) # Clamped in the x assignment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0  Progression with MPC_T= 15 :  0.0551\n",
      "tensor([0.0000, 1.0200, 0.9800, 0.9800, 0.0000, 0.9800, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.9800, 0.9800], grad_fn=<CatBackward0>)\n",
      "Batch: 1  Progression with MPC_T= 15 :  0.0741\n",
      "tensor([0.0000, 1.0254, 0.9998, 0.9635, 0.0000, 0.9637, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.9633, 0.9721], grad_fn=<CatBackward0>)\n",
      "Batch: 2  Progression with MPC_T= 15 :  0.1028\n",
      "tensor([0.0000, 1.0244, 1.0091, 0.9481, 0.0000, 0.9497, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.9457, 0.9540], grad_fn=<CatBackward0>)\n",
      "Batch: 3  Progression with MPC_T= 15 :  0.0739\n",
      "tensor([0.0000, 1.0163, 1.0232, 0.9385, 0.0000, 0.9413, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.9364, 0.9371], grad_fn=<CatBackward0>)\n",
      "Batch: 4  Progression with MPC_T= 15 :  0.1282\n",
      "tensor([0.0000, 1.0118, 1.0287, 0.9256, 0.0000, 0.9281, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.9222, 0.9279], grad_fn=<CatBackward0>)\n",
      "Batch: 5  Progression with MPC_T= 15 :  0.0551\n",
      "tensor([0.0000, 1.0304, 1.0397, 0.9202, 0.0000, 0.9225, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.9185, 0.9160], grad_fn=<CatBackward0>)\n",
      "Batch: 6  Progression with MPC_T= 15 :  0.13\n",
      "tensor([0.0000, 1.0281, 1.0427, 0.9092, 0.0000, 0.9109, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.9083, 0.9085], grad_fn=<CatBackward0>)\n",
      "Batch: 7  Progression with MPC_T= 15 :  0.1442\n",
      "tensor([0.0000, 1.0121, 1.0529, 0.8985, 0.0000, 0.8987, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.8976, 0.9038], grad_fn=<CatBackward0>)\n",
      "Batch: 8  Progression with MPC_T= 15 :  0.1348\n",
      "tensor([0.0000, 1.0156, 1.0594, 0.8895, 0.0000, 0.8900, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.8899, 0.8955], grad_fn=<CatBackward0>)\n",
      "Batch: 9  Progression with MPC_T= 15 :  0.145\n",
      "tensor([0.0000, 1.0158, 1.0683, 0.8805, 0.0000, 0.8814, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.8795, 0.8844], grad_fn=<CatBackward0>)\n",
      "Batch: 10  Progression with MPC_T= 15 :  0.1082\n",
      "tensor([0.0000, 1.0131, 1.0778, 0.8743, 0.0000, 0.8759, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.8752, 0.8703], grad_fn=<CatBackward0>)\n",
      "Batch: 11  Progression with MPC_T= 15 :  0.0905\n",
      "tensor([0.0000, 1.0186, 1.0845, 0.8692, 0.0000, 0.8706, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.8712, 0.8632], grad_fn=<CatBackward0>)\n",
      "Batch: 12  Progression with MPC_T= 15 :  0.1197\n",
      "tensor([0.0000, 1.0198, 1.0983, 0.8619, 0.0000, 0.8645, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.8625, 0.8532], grad_fn=<CatBackward0>)\n",
      "Batch: 13  Progression with MPC_T= 15 :  0.0778\n",
      "tensor([0.0000, 1.0234, 1.1028, 0.8578, 0.0000, 0.8607, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.8593, 0.8449], grad_fn=<CatBackward0>)\n",
      "Batch: 14  Progression with MPC_T= 15 :  0.0868\n",
      "tensor([0.0000, 1.0297, 1.1059, 0.8533, 0.0000, 0.8557, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.8553, 0.8424], grad_fn=<CatBackward0>)\n",
      "Batch: 15  Progression with MPC_T= 15 :  0.0921\n",
      "tensor([0.0000, 1.0371, 1.1099, 0.8485, 0.0000, 0.8504, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.8515, 0.8389], grad_fn=<CatBackward0>)\n",
      "Batch: 16  Progression with MPC_T= 15 :  0.1415\n",
      "tensor([0.0000, 1.0343, 1.1122, 0.8418, 0.0000, 0.8436, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.8429, 0.8320], grad_fn=<CatBackward0>)\n",
      "Batch: 17  Progression with MPC_T= 15 :  0.0912\n",
      "tensor([0.0000, 1.0319, 1.1146, 0.8375, 0.0000, 0.8390, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.8391, 0.8276], grad_fn=<CatBackward0>)\n",
      "Batch: 18  Progression with MPC_T= 15 :  0.121\n",
      "tensor([0.0000, 1.0155, 1.1203, 0.8322, 0.0000, 0.8302, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.8340, 0.8231], grad_fn=<CatBackward0>)\n",
      "Batch: 19  Progression with MPC_T= 15 :  0.1048\n",
      "tensor([0.0000, 1.0188, 1.1243, 0.8275, 0.0000, 0.8254, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.8298, 0.8162], grad_fn=<CatBackward0>)\n",
      "Batch: 20  Progression with MPC_T= 15 :  0.0886\n",
      "tensor([0.0000, 1.0241, 1.1296, 0.8233, 0.0000, 0.8218, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.8252, 0.8103], grad_fn=<CatBackward0>)\n",
      "Batch: 21  Progression with MPC_T= 15 :  0.1233\n",
      "tensor([0.0000, 1.0266, 1.1334, 0.8177, 0.0000, 0.8161, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.8203, 0.8066], grad_fn=<CatBackward0>)\n",
      "Batch: 22  Progression with MPC_T= 15 :  0.1656\n",
      "tensor([0.0000, 1.0275, 1.1370, 0.8111, 0.0000, 0.8087, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.8142, 0.7997], grad_fn=<CatBackward0>)\n",
      "Batch: 23  Progression with MPC_T= 15 :  0.1195\n",
      "tensor([0.0000, 1.0295, 1.1404, 0.8059, 0.0000, 0.8039, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.8085, 0.7969], grad_fn=<CatBackward0>)\n",
      "Batch: 24  Progression with MPC_T= 15 :  0.098\n",
      "tensor([0.0000, 1.0289, 1.1424, 0.8017, 0.0000, 0.8004, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.8040, 0.7919], grad_fn=<CatBackward0>)\n",
      "Batch: 25  Progression with MPC_T= 15 :  0.1852\n",
      "tensor([0.0000, 1.0140, 1.1520, 0.7938, 0.0000, 0.7920, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.7941, 0.7850], grad_fn=<CatBackward0>)\n",
      "Batch: 26  Progression with MPC_T= 15 :  0.1243\n",
      "tensor([0.0000, 1.0140, 1.1530, 0.7894, 0.0000, 0.7873, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.7902, 0.7792], grad_fn=<CatBackward0>)\n",
      "Batch: 27  Progression with MPC_T= 15 :  0.2145\n",
      "tensor([0.0000, 1.0052, 1.1573, 0.7821, 0.0000, 0.7781, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.7825, 0.7745], grad_fn=<CatBackward0>)\n",
      "Batch: 28  Progression with MPC_T= 15 :  0.1029\n",
      "tensor([0.0000, 1.0043, 1.1586, 0.7783, 0.0000, 0.7747, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.7788, 0.7703], grad_fn=<CatBackward0>)\n",
      "Batch: 29  Progression with MPC_T= 15 :  0.1249\n",
      "tensor([0.0000, 1.0035, 1.1637, 0.7738, 0.0000, 0.7707, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.7742, 0.7643], grad_fn=<CatBackward0>)\n",
      "Batch: 30  Progression with MPC_T= 15 :  0.1034\n",
      "tensor([0.0000, 1.0052, 1.1681, 0.7700, 0.0000, 0.7676, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.7702, 0.7573], grad_fn=<CatBackward0>)\n",
      "Batch: 31  Progression with MPC_T= 15 :  0.1217\n",
      "tensor([0.0000, 1.0037, 1.1667, 0.7660, 0.0000, 0.7633, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.7661, 0.7517], grad_fn=<CatBackward0>)\n",
      "Batch: 32  Progression with MPC_T= 15 :  0.1229\n",
      "tensor([0.0000, 1.0022, 1.1710, 0.7618, 0.0000, 0.7594, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.7614, 0.7403], grad_fn=<CatBackward0>)\n",
      "Batch: 33  Progression with MPC_T= 15 :  0.1334\n",
      "tensor([0.0000, 1.0057, 1.1755, 0.7572, 0.0000, 0.7551, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.7568, 0.7362], grad_fn=<CatBackward0>)\n",
      "Batch: 34  Progression with MPC_T= 15 :  0.137\n",
      "tensor([0.0000, 1.0056, 1.1793, 0.7528, 0.0000, 0.7506, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.7523, 0.7291], grad_fn=<CatBackward0>)\n",
      "Batch: 35  Progression with MPC_T= 15 :  0.1459\n",
      "tensor([0.0000, 1.0049, 1.1821, 0.7481, 0.0000, 0.7459, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.7481, 0.7258], grad_fn=<CatBackward0>)\n",
      "Batch: 36  Progression with MPC_T= 15 :  0.1256\n",
      "tensor([0.0000, 1.0065, 1.1876, 0.7438, 0.0000, 0.7424, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.7434, 0.7222], grad_fn=<CatBackward0>)\n",
      "Batch: 37  Progression with MPC_T= 15 :  0.1288\n",
      "tensor([0.0000, 1.0066, 1.1878, 0.7400, 0.0000, 0.7385, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.7398, 0.7179], grad_fn=<CatBackward0>)\n",
      "Batch: 38  Progression with MPC_T= 15 :  0.112\n",
      "tensor([0.0000, 1.0085, 1.1899, 0.7362, 0.0000, 0.7353, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.7352, 0.7149], grad_fn=<CatBackward0>)\n",
      "Batch: 39  Progression with MPC_T= 15 :  0.1537\n",
      "tensor([0.0000, 1.0089, 1.1938, 0.7316, 0.0000, 0.7306, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.7307, 0.7089], grad_fn=<CatBackward0>)\n",
      "Batch: 40  Progression with MPC_T= 15 :  0.1305\n",
      "tensor([0.0000, 1.0091, 1.2003, 0.7275, 0.0000, 0.7269, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.7261, 0.7005], grad_fn=<CatBackward0>)\n",
      "Batch: 41  Progression with MPC_T= 15 :  0.2073\n",
      "tensor([0.0000, 1.0044, 1.2006, 0.7216, 0.0000, 0.7200, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.7198, 0.6954], grad_fn=<CatBackward0>)\n",
      "Batch: 42  Progression with MPC_T= 15 :  0.1873\n",
      "tensor([0.0000, 1.0015, 1.2036, 0.7162, 0.0000, 0.7141, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.7146, 0.6907], grad_fn=<CatBackward0>)\n",
      "Batch: 43  Progression with MPC_T= 15 :  0.1448\n",
      "tensor([0.0000, 0.9991, 1.2077, 0.7123, 0.0000, 0.7100, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.7101, 0.6854], grad_fn=<CatBackward0>)\n",
      "Batch: 44  Progression with MPC_T= 15 :  0.1561\n",
      "tensor([0.0000, 1.0007, 1.2128, 0.7077, 0.0000, 0.7057, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.7053, 0.6804], grad_fn=<CatBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 45  Progression with MPC_T= 15 :  0.1775\n",
      "tensor([0.0000, 0.9999, 1.2188, 0.7027, 0.0000, 0.7007, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.7002, 0.6769], grad_fn=<CatBackward0>)\n",
      "Batch: 46  Progression with MPC_T= 15 :  0.181\n",
      "tensor([0.0000, 0.9945, 1.2227, 0.6978, 0.0000, 0.6951, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.6948, 0.6741], grad_fn=<CatBackward0>)\n",
      "Batch: 47  Progression with MPC_T= 15 :  0.1997\n",
      "tensor([0.0000, 0.9903, 1.2295, 0.6924, 0.0000, 0.6892, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.6890, 0.6705], grad_fn=<CatBackward0>)\n",
      "Batch: 48  Progression with MPC_T= 15 :  0.1559\n",
      "tensor([0.0000, 0.9924, 1.2312, 0.6882, 0.0000, 0.6852, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.6847, 0.6680], grad_fn=<CatBackward0>)\n",
      "Batch: 49  Progression with MPC_T= 15 :  0.1708\n",
      "tensor([0.0000, 0.9943, 1.2344, 0.6837, 0.0000, 0.6809, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.6797, 0.6628], grad_fn=<CatBackward0>)\n",
      "Batch: 50  Progression with MPC_T= 15 :  0.2098\n",
      "tensor([0.0000, 0.9959, 1.2388, 0.6784, 0.0000, 0.6753, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.6741, 0.6588], grad_fn=<CatBackward0>)\n",
      "Batch: 51  Progression with MPC_T= 15 :  0.1921\n",
      "tensor([0.0000, 0.9952, 1.2414, 0.6738, 0.0000, 0.6705, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.6691, 0.6543], grad_fn=<CatBackward0>)\n",
      "Batch: 52  Progression with MPC_T= 15 :  0.1995\n",
      "tensor([0.0000, 0.9911, 1.2456, 0.6690, 0.0000, 0.6654, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.6637, 0.6484], grad_fn=<CatBackward0>)\n",
      "Batch: 53  Progression with MPC_T= 15 :  0.252\n",
      "tensor([0.0000, 0.9813, 1.2500, 0.6626, 0.0000, 0.6587, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.6567, 0.6446], grad_fn=<CatBackward0>)\n",
      "Batch: 54  Progression with MPC_T= 15 :  0.1922\n",
      "tensor([0.0000, 0.9816, 1.2531, 0.6579, 0.0000, 0.6545, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.6518, 0.6439], grad_fn=<CatBackward0>)\n",
      "Batch: 55  Progression with MPC_T= 15 :  0.1763\n",
      "tensor([0.0000, 0.9779, 1.2553, 0.6537, 0.0000, 0.6503, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.6473, 0.6410], grad_fn=<CatBackward0>)\n",
      "Batch: 56  Progression with MPC_T= 15 :  0.1897\n",
      "tensor([0.0000, 0.9775, 1.2596, 0.6493, 0.0000, 0.6462, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.6424, 0.6392], grad_fn=<CatBackward0>)\n",
      "Batch: 57  Progression with MPC_T= 15 :  0.1926\n",
      "tensor([0.0000, 0.9740, 1.2618, 0.6449, 0.0000, 0.6415, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.6376, 0.6358], grad_fn=<CatBackward0>)\n",
      "Batch: 58  Progression with MPC_T= 15 :  0.2134\n",
      "tensor([0.0000, 0.9786, 1.2653, 0.6402, 0.0000, 0.6370, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.6327, 0.6317], grad_fn=<CatBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m x_init \u001b[38;5;241m=\u001b[39m sample_xinit(n_batch)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#im_loss = get_loss_cost(x_init, dx, Q_batch, p_batch)\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m im_loss \u001b[38;5;241m=\u001b[39m \u001b[43mget_loss_progress\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_init\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mQ_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     18\u001b[0m im_loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36mget_loss_progress\u001b[0;34m(x_init, dx, _Q, _p, mpc_T)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_loss_progress\u001b[39m(x_init, dx, _Q, _p, mpc_T\u001b[38;5;241m=\u001b[39mmpc_T):    \n\u001b[0;32m----> 8\u001b[0m         pred_x, pred_u, pred_objs \u001b[38;5;241m=\u001b[39m \u001b[43mmpc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMPC\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_ctrl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmpc_T\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m            \u001b[49m\u001b[43mu_lower\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mu_lower\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu_upper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mu_upper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mu_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlqr_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlqr_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexit_unconverged\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdetach_unconverged\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlinesearch_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinesearch_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_linesearch_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_linesearch_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgrad_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m            \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_init\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mQuadCost\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_Q\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_p\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;66;03m#I added the second term to account the initial state (to kind of normalize the progress)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m         progress_loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m-\u001b[39mpred_x[mpc_T\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,:,\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m pred_x[\u001b[38;5;241m0\u001b[39m,:,\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/Desktop/envs/pao_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/envs/pao_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Research/Code/race_application/mpc/mpc.py:265\u001b[0m, in \u001b[0;36mMPC.forward\u001b[0;34m(self, x_init, cost, dx)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    261\u001b[0m     C, c, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapproximate_cost(\n\u001b[1;32m    262\u001b[0m         x, util\u001b[38;5;241m.\u001b[39mdetach_maybe(u), cost, diff\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    264\u001b[0m x, u, n_total_qp_iter, costs, full_du_norm, mean_alphas \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m--> 265\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolve_lqr_subproblem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_init\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m n_not_improved \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndimension() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/Research/Code/race_application/mpc/mpc.py:361\u001b[0m, in \u001b[0;36mMPC.solve_lqr_subproblem\u001b[0;34m(self, x_init, C, c, F, f, cost, dynamics, x, u, no_op_forward)\u001b[0m\n\u001b[1;32m    342\u001b[0m     _lqr \u001b[38;5;241m=\u001b[39m LQRStep(\n\u001b[1;32m    343\u001b[0m         n_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_state,\n\u001b[1;32m    344\u001b[0m         n_ctrl\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_ctrl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    358\u001b[0m         no_op_forward\u001b[38;5;241m=\u001b[39mno_op_forward,\n\u001b[1;32m    359\u001b[0m     )\n\u001b[1;32m    360\u001b[0m     e \u001b[38;5;241m=\u001b[39m Variable(torch\u001b[38;5;241m.\u001b[39mTensor())\n\u001b[0;32m--> 361\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_lqr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_init\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    363\u001b[0m     nsc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_state \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_ctrl\n",
      "File \u001b[0;32m~/Desktop/envs/pao_env/lib/python3.8/site-packages/torch/autograd/function.py:553\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    552\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    557\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    558\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    559\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    560\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    561\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/Research/Code/race_application/mpc/lqr_step.py:304\u001b[0m, in \u001b[0;36mLQRStep.<locals>.LQRStepFn.forward\u001b[0;34m(ctx, x_init, C, c, F, f)\u001b[0m\n\u001b[1;32m    301\u001b[0m ctx\u001b[38;5;241m.\u001b[39mcurrent_u \u001b[38;5;241m=\u001b[39m current_u\n\u001b[1;32m    303\u001b[0m Ks, ks, n_total_qp_iter \u001b[38;5;241m=\u001b[39m lqr_backward(ctx, C, c_back, F, f_back)\n\u001b[0;32m--> 304\u001b[0m new_x, new_u, for_out \u001b[38;5;241m=\u001b[39m \u001b[43mlqr_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_init\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mKs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    306\u001b[0m ctx\u001b[38;5;241m.\u001b[39msave_for_backward(x_init, C, c, F, f, new_x, new_u)\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_x, new_u, torch\u001b[38;5;241m.\u001b[39mTensor([n_total_qp_iter]), \\\n\u001b[1;32m    309\u001b[0m   for_out\u001b[38;5;241m.\u001b[39mcosts, for_out\u001b[38;5;241m.\u001b[39mfull_du_norm, for_out\u001b[38;5;241m.\u001b[39mmean_alphas\n",
      "File \u001b[0;32m~/Desktop/Research/Code/race_application/mpc/lqr_step.py:216\u001b[0m, in \u001b[0;36mLQRStep.<locals>.lqr_forward\u001b[0;34m(ctx, x_init, C, c, F, f, Ks, ks)\u001b[0m\n\u001b[1;32m    213\u001b[0m     new_ut \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39meclamp(new_ut, lb, ub)\n\u001b[1;32m    214\u001b[0m new_u\u001b[38;5;241m.\u001b[39mappend(new_ut)\n\u001b[0;32m--> 216\u001b[0m new_xut \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_xt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_ut\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;241m<\u001b[39m T\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(true_dynamics, mpc\u001b[38;5;241m.\u001b[39mLinDx):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(150):\n",
    "\n",
    "    q = torch.cat((q_sigma,learn_q_logit_state,q_sigma_0,learn_q_logit_sigma_diff,q_penalty,learn_q_logit_input),dim=0)\n",
    "    p = torch.cat((p_sigma,learn_p_state,p_sigma_0,learn_p_sigma_diff,p_penalty,learn_p_input), dim=0)\n",
    "    #print(q)\n",
    "    print(q)\n",
    "\n",
    "    Q_batch = torch.diag(q).unsqueeze(0).unsqueeze(0).repeat(\n",
    "                mpc_T, n_batch, 1, 1\n",
    "            )\n",
    "    p_batch = p.unsqueeze(0).repeat(mpc_T, n_batch, 1)\n",
    "\n",
    "    x_init = sample_xinit(n_batch).to(device)\n",
    "    #im_loss = get_loss_cost(x_init, dx, Q_batch, p_batch)\n",
    "    im_loss = get_loss_progress(x_init, dx, Q_batch, p_batch)\n",
    "\n",
    "    opt.zero_grad()\n",
    "    im_loss.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    print('Batch:', i , ' Progression with MPC_T=',mpc_T ,': ', -round(im_loss.item(), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we can choose the number of samples we want to test (number of initial states)\n",
    "N_test = 1\n",
    "\n",
    "# Whatever I wrote below might be wrong, we have to see if we really can change the mpc_T to Test, \n",
    "# it gives weird results sometimes.\n",
    "# Here we can choose the mpc_T in the test time, which can be much higher than in the training.\n",
    "# Ideally, we would like to have the whole lap here, I guess. But we need to fix the warnings/errors before.\n",
    "mpc_T_test = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_init_test = sample_xinit(N_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below you can put any initial state you want (any that make sense)\n",
    "#x_init_test = torch.tensor([[1.6, 0.1, -0.8, 0.1,1.6,0,softplus_op(torch.Tensor([-0.1+0.0])), softplus_op(torch.Tensor([-0.1-0.5*track_width])),softplus_op(torch.Tensor([0.1-0.5*track_width])), softplus_op(torch.Tensor([-0.1-0.5*track_width]))]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_test = torch.diag(q).unsqueeze(0).unsqueeze(0).repeat(\n",
    "                mpc_T_test, N_test, 1, 1\n",
    "            )\n",
    "p_test = p.unsqueeze(0).repeat(mpc_T_test, N_test, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_q, true_p = true_dx.get_true_obj()\n",
    "true_q = torch.Tensor([ 0.,  3.,  1.,  0., 0., 0., 1., 2.])\n",
    "true_p = torch.Tensor([ -2.,  0.,  0.,  0., 100., 100.,  -1,  0.])\n",
    "\n",
    "true_q = true_q.to(device)\n",
    "true_p = true_p.to(device)\n",
    "\n",
    "expert_Q = torch.diag(true_q).unsqueeze(0).unsqueeze(0).repeat(\n",
    "            mpc_T_test, N_test, 1, 1\n",
    "        )\n",
    "expert_p = true_p.unsqueeze(0).repeat(mpc_T_test, N_test, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Added here the bounds of U\n",
    "u_lower_test = torch.tensor([-2., -1.]).unsqueeze(0).unsqueeze(0).repeat(mpc_T_test, N_test, 1)\n",
    "u_upper_test = torch.tensor([2., 1.]).unsqueeze(0).unsqueeze(0).repeat(mpc_T_test, N_test, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mpc_test, u_mpc_test, objs_mpc_test = mpc.MPC(\n",
    "            n_state, n_ctrl, mpc_T_test,\n",
    "            u_lower=u_lower_test, u_upper=u_upper_test, u_init=u_init,\n",
    "            lqr_iter=lqr_iter,\n",
    "            verbose=0,\n",
    "            exit_unconverged=False,\n",
    "            detach_unconverged=True,\n",
    "            linesearch_decay=dx.linesearch_decay,\n",
    "            max_linesearch_iter=dx.max_linesearch_iter,\n",
    "            grad_method=grad_method,\n",
    "            eps=2,\n",
    "            n_batch=N_test,\n",
    "        )(x_init_test, QuadCost(Q_test, p_test), dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frenet_to_cartesian(point_f, ref_path):\n",
    "    \n",
    "    def get_nearest_index(point_f, ref_path):\n",
    "        return ((point_f[0] - ref_path[2,:])**2).argmin()\n",
    "    \n",
    "    def compute_x_coord(point_f, ref_path, nearest_index):\n",
    "        return ref_path[0,nearest_index] - point_f[1]*torch.sin(ref_path[3,nearest_index])\n",
    "    \n",
    "    def compute_y_coord(point_f, ref_path, nearest_index):\n",
    "        return ref_path[1,nearest_index] + point_f[1]*torch.cos(ref_path[3,nearest_index])\n",
    "    \n",
    "    nearest_index = get_nearest_index(point_f, ref_path)\n",
    "    x = compute_x_coord(point_f, ref_path, nearest_index)\n",
    "    y = compute_y_coord(point_f, ref_path, nearest_index)\n",
    "    \n",
    "    return torch.tensor([x, y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_list = []\n",
    "y_list = []\n",
    "\n",
    "for i in range(mpc_T_test):\n",
    "    xy = frenet_to_cartesian(x_mpc_test[i,0,:2], track_coord)\n",
    "    x_list.append(xy[0].numpy())\n",
    "    y_list.append(xy[1].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_plot = np.array(x_list)\n",
    "y_plot = np.array(y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7880, 0.3495], grad_fn=<MaxBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_mpc_test.max(0)[0].max(0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_init: 0.15000000000000002\n",
      "y_init: -1.0499999999999994\n",
      "yaw_init: 0.0\n",
      "Total Arc Length: 11.568244517641709\n"
     ]
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(10,5), dpi=120)\n",
    "gen.plotPoints(ax)\n",
    "#gen.pointAtArcLength(0)\n",
    "#gen.writePointsToYaml('../tracks/' + track_name + '.yaml', track_density)\n",
    "\n",
    "ax.scatter(x_plot, y_plot, s=4, color='red')\n",
    "\n",
    "print('x_init: ' + str(gen.xCoords[0]))\n",
    "print('y_init: ' + str(gen.yCoords[0]))\n",
    "print('yaw_init: ' + str(gen.tangentAngle[0]))\n",
    "print('Total Arc Length: ' + str(gen.arcLength[-1]/2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pao_env",
   "language": "python",
   "name": "pao_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
