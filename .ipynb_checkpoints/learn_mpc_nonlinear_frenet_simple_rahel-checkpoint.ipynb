{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Function, Variable\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils import parameters_to_vector\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Comment these lines if your MPC is in the current directory.\n",
    "# Otherwise modify to the directory.\n",
    "#import sys\n",
    "#sys.path.append('./../racingDiffMPC/')\n",
    "\n",
    "\n",
    "from mpc import mpc\n",
    "from mpc import casadi_control\n",
    "from mpc.mpc import GradMethods, QuadCost, LinDx\n",
    "#from mpc.dynamics import NNDynamics\n",
    "#import mpc.util as eutil\n",
    "from mpc.env_dx import frenet_kin_bicycle\n",
    "from mpc.track.src import simple_track_generator, track_functions\n",
    "\n",
    "\n",
    "\n",
    "#import sys\n",
    "#from IPython.core import ultratb\n",
    "#sys.excepthook = ultratb.FormattedTB(mode='Verbose',\n",
    "#     color_scheme='Linux', call_pdb=1)\n",
    "\n",
    "import time\n",
    "import os\n",
    "import shutil\n",
    "import pickle as pkl\n",
    "import collections\n",
    "\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try to create a track \n",
    "\n",
    "track_density = 300\n",
    "track_width = 0.5\n",
    "gen = simple_track_generator.trackGenerator(track_density,track_width)\n",
    "track_name = 'DEMO_TRACK'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 0.3\n",
    "init = [0,0,0]\n",
    "\n",
    "track_function = {\n",
    "    'DEMO_TRACK'    : track_functions.demo_track,\n",
    "    'HARD_TRACK'    : track_functions.hard_track,\n",
    "    'LONG_TRACK'    : track_functions.long_track,\n",
    "    'LUCERNE_TRACK' : track_functions.lucerne_track,\n",
    "    'BERN_TRACK'    : track_functions.bern_track,\n",
    "    'INFINITY_TRACK': track_functions.infinity_track,\n",
    "    'SNAIL_TRACK'   : track_functions.snail_track\n",
    "}.get(track_name, track_functions.demo_track)\n",
    "    \n",
    "track_function(gen, t, init)\n",
    "    \n",
    "gen.populatePointsAndArcLength()\n",
    "gen.centerTrack()\n",
    "\n",
    "track_coord = torch.from_numpy(np.vstack([gen.xCoords, gen.yCoords, gen.arcLength, gen.tangentAngle, gen.curvature]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "softplus_op = torch.nn.Softplus(10)\n",
    "\n",
    "def sample_xinit(n_batch):\n",
    "    def uniform(shape, low, high):\n",
    "        r = high-low\n",
    "        return torch.rand(shape)*r+low\n",
    "\n",
    "    sigma = uniform(n_batch, 0.01, 2.)\n",
    "    d = uniform(n_batch, -0.1, 0.1)\n",
    "    phi = uniform(n_batch, -0.40*np.pi, 0.40*np.pi)\n",
    "    v = uniform(n_batch, 0., 0.2)\n",
    "    d_lb = softplus_op(-d-0.5*track_width)\n",
    "    d_ub = softplus_op(d-0.5*track_width)\n",
    "    xinit = torch.stack((sigma, d, phi, v, d_lb, d_ub), dim=1)\n",
    "\n",
    "    return xinit\n",
    "\n",
    "true_dx = frenet_kin_bicycle.FrenetKinBicycleDx(track_coord)\n",
    "mpc_T = 15\n",
    "n_batch = 8\n",
    "\n",
    "# Added here the bounds of U\n",
    "u_lower = torch.tensor([-2., -1.]).unsqueeze(0).unsqueeze(0).repeat(mpc_T, n_batch, 1)\n",
    "u_upper = torch.tensor([2., 1.]).unsqueeze(0).unsqueeze(0).repeat(mpc_T, n_batch, 1)\n",
    "\n",
    "n_state = true_dx.n_state\n",
    "n_ctrl = true_dx.n_ctrl\n",
    "\n",
    "u_init=None\n",
    "eps = 1\n",
    "lqr_iter = 500\n",
    "grad_method = GradMethods.AUTO_DIFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# casadi mpc with exact penalty\n",
    "test_q = np.array([ 0.,  6.,  1.,  0., 0., 0., 1., 2.])\n",
    "test_p = np.array([ -2.,  0.,  0.,  0., 100., 100.,  -1,  0.])\n",
    "\n",
    "control = casadi_control.CasadiControl(track_coord)\n",
    "\n",
    "x0 = (sample_xinit(1)).numpy()\n",
    "print(x0[0])\n",
    "dc = 2 #number constraints\n",
    "dx = n_state #number states\n",
    "du = n_ctrl #number control inputs\n",
    "horizon = mpc_T\n",
    "sol = control.mpc_casadi(test_q,test_p,x0,horizon,dc,dx,du,track_width)\n",
    "N = horizon\n",
    "u = sol[-du*N:]\n",
    "x = sol[:-du*N]\n",
    "u_r = u.reshape(N,du)\n",
    "x_r = x.reshape(N+1,dx-dc)\n",
    "print(u_r)\n",
    "print(x_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#casadi mpc with state constraints - NOTE: cost parameter vector has now different dimensions\n",
    "test_q = np.array([ 0.,  6.,  1.,  0., 1., 2.])\n",
    "test_p = np.array([ -2.,  0.,  0.,  0.,  -1,  0.])\n",
    "\n",
    "sol = control.mpc_casadi_with_constraints(test_q,test_p,x0,horizon,dc,dx,du,track_width)\n",
    "N = horizon\n",
    "u = sol[-du*N:]\n",
    "x = sol[:-du*N]\n",
    "u_r = u.reshape(N,du)\n",
    "x_r = x.reshape(N+1,dx-dc)\n",
    "print(u_r)\n",
    "print(x_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each mpc returns a prediction of the states and the inputs over the horizon for all batches and all dimensions,\n",
    "# they are stored in the following order pred_x = [horizon,batch, dimension].\n",
    "# To penalize a control behaviour that does not enforce enough progress within a horizon we can try to chose \n",
    "# our loss as -pred_x[mpc_T-1,:,0]\n",
    "\n",
    "def get_loss_progress(x_init, dx, _Q, _p, mpc_T=mpc_T):    \n",
    "        \n",
    "        pred_x, pred_u, pred_objs = mpc.MPC(\n",
    "            dx.n_state, dx.n_ctrl, mpc_T,\n",
    "            u_lower=u_lower, u_upper=u_upper, u_init=u_init,\n",
    "            lqr_iter=lqr_iter,\n",
    "            verbose=0,\n",
    "            exit_unconverged=False,\n",
    "            detach_unconverged=True,\n",
    "            linesearch_decay=dx.linesearch_decay,\n",
    "            max_linesearch_iter=dx.max_linesearch_iter,\n",
    "            grad_method=grad_method,\n",
    "            eps=0.5,\n",
    "            n_batch=n_batch,\n",
    "        )(x_init, QuadCost(_Q, _p), dx)\n",
    "        \n",
    "        #I added the second term to account the initial state (to kind of normalize the progress)\n",
    "        progress_loss = torch.mean(-pred_x[mpc_T-1,:,0] + pred_x[0,:,0])\n",
    "        \n",
    "        #penalty_loss = pred_x[]\n",
    "            \n",
    "        return progress_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params1 = []\n",
    "learn_q_logit_state = torch.ones(n_state-2, requires_grad=True).to(device)\n",
    "learn_q_logit_input = torch.ones(n_ctrl, requires_grad=True).to(device)\n",
    "learn_p_state = torch.zeros(n_state-2, requires_grad=True).to(device)\n",
    "learn_p_input = torch.zeros(n_ctrl, requires_grad=True).to(device)\n",
    "params1 += [learn_q_logit_state, learn_q_logit_input, learn_p_state, learn_p_input]\n",
    "env_params = true_dx.params\n",
    "q_penalty = .01*torch.ones(2).to(device)\n",
    "p_penalty = 10*torch.ones(2).to(device)\n",
    "track_coord = track_coord.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn_q_logit = torch.ones_like(true_q, requires_grad=True).to(device)\n",
    "#learn_p = torch.zeros_like(true_p, requires_grad=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = [{\n",
    "            'params': params1,\n",
    "            'lr': 2e-3,\n",
    "            'alpha': 0.99,\n",
    "            }]\n",
    "dx = true_dx.__class__(track_coord,env_params)\n",
    "\n",
    "opt = optim.RMSprop(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(150):\n",
    "\n",
    "    q = torch.cat((learn_q_logit_state,q_penalty,learn_q_logit_input),dim=0)\n",
    "    p = torch.cat((learn_p_state,p_penalty,learn_p_input), dim=0)\n",
    "    #print(q)\n",
    "\n",
    "    Q_batch = torch.diag(q).unsqueeze(0).unsqueeze(0).repeat(\n",
    "                mpc_T, n_batch, 1, 1\n",
    "            )\n",
    "    p_batch = p.unsqueeze(0).repeat(mpc_T, n_batch, 1)\n",
    "\n",
    "    x_init = sample_xinit(n_batch).to(device)\n",
    "    #im_loss = get_loss_cost(x_init, dx, Q_batch, p_batch)\n",
    "    im_loss = get_loss_progress(x_init, dx, Q_batch, p_batch)\n",
    "\n",
    "    opt.zero_grad()\n",
    "    im_loss.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    print('Batch:', i , ' Progression with MPC_T=',mpc_T ,': ', -round(im_loss.item(), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we can choose the number of samples we want to test (number of initial states)\n",
    "N_test = 1\n",
    "\n",
    "# Whatever I wrote below might be wrong, we have to see if we really can change the mpc_T to Test, \n",
    "# it gives weird results sometimes.\n",
    "# Here we can choose the mpc_T in the test time, which can be much higher than in the training.\n",
    "# Ideally, we would like to have the whole lap here, I guess. But we need to fix the warnings/errors before.\n",
    "mpc_T_test = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_init_test = sample_xinit(N_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below you can put any initial state you want (any that make sense)\n",
    "x_init_test = torch.tensor([[1.6, 0.1, -0.8, 0.1,softplus_op(torch.Tensor([-0.1-0.5*track_width])), softplus_op(torch.Tensor([-0.1-0.5*track_width]))]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_test = torch.diag(q).unsqueeze(0).unsqueeze(0).repeat(\n",
    "                mpc_T_test, N_test, 1, 1\n",
    "            )\n",
    "p_test = p.unsqueeze(0).repeat(mpc_T_test, N_test, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_q, true_p = true_dx.get_true_obj()\n",
    "true_q = torch.Tensor([ 0.,  3.,  1.,  0., 0., 0., 1., 2.])\n",
    "true_p = torch.Tensor([ -2.,  0.,  0.,  0., 100., 100.,  -1,  0.])\n",
    "\n",
    "true_q = true_q.to(device)\n",
    "true_p = true_p.to(device)\n",
    "\n",
    "expert_Q = torch.diag(true_q).unsqueeze(0).unsqueeze(0).repeat(\n",
    "            mpc_T_test, N_test, 1, 1\n",
    "        )\n",
    "expert_p = true_p.unsqueeze(0).repeat(mpc_T_test, N_test, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Added here the bounds of U\n",
    "u_lower_test = torch.tensor([-2., -1.]).unsqueeze(0).unsqueeze(0).repeat(mpc_T_test, N_test, 1)\n",
    "u_upper_test = torch.tensor([2., 1.]).unsqueeze(0).unsqueeze(0).repeat(mpc_T_test, N_test, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mpc_test, u_mpc_test, objs_mpc_test = mpc.MPC(\n",
    "            n_state, n_ctrl, mpc_T_test,\n",
    "            u_lower=u_lower_test, u_upper=u_upper_test, u_init=u_init,\n",
    "            lqr_iter=lqr_iter,\n",
    "            verbose=0,\n",
    "            exit_unconverged=False,\n",
    "            detach_unconverged=True,\n",
    "            linesearch_decay=dx.linesearch_decay,\n",
    "            max_linesearch_iter=dx.max_linesearch_iter,\n",
    "            grad_method=grad_method,\n",
    "            eps=eps,\n",
    "            n_batch=N_test,\n",
    "        )(x_init_test, QuadCost(expert_Q, expert_p), dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frenet_to_cartesian(point_f, ref_path):\n",
    "    \n",
    "    def get_nearest_index(point_f, ref_path):\n",
    "        return ((point_f[0] - ref_path[2,:])**2).argmin()\n",
    "    \n",
    "    def compute_x_coord(point_f, ref_path, nearest_index):\n",
    "        return ref_path[0,nearest_index] - point_f[1]*torch.sin(ref_path[3,nearest_index])\n",
    "    \n",
    "    def compute_y_coord(point_f, ref_path, nearest_index):\n",
    "        return ref_path[1,nearest_index] + point_f[1]*torch.cos(ref_path[3,nearest_index])\n",
    "    \n",
    "    nearest_index = get_nearest_index(point_f, ref_path)\n",
    "    x = compute_x_coord(point_f, ref_path, nearest_index)\n",
    "    y = compute_y_coord(point_f, ref_path, nearest_index)\n",
    "    \n",
    "    return torch.tensor([x, y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_list = []\n",
    "y_list = []\n",
    "\n",
    "for i in range(mpc_T_test):\n",
    "    xy = frenet_to_cartesian(x_mpc_test[i,0,:2], track_coord)\n",
    "    x_list.append(xy[0].numpy())\n",
    "    y_list.append(xy[1].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_plot = np.array(x_list)\n",
    "y_plot = np.array(y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_mpc_test.max(0)[0].max(0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(10,5), dpi=120)\n",
    "gen.plotPoints(ax)\n",
    "#gen.pointAtArcLength(0)\n",
    "#gen.writePointsToYaml('../tracks/' + track_name + '.yaml', track_density)\n",
    "\n",
    "ax.scatter(x_plot, y_plot, s=4, color='red')\n",
    "\n",
    "print('x_init: ' + str(gen.xCoords[0]))\n",
    "print('y_init: ' + str(gen.yCoords[0]))\n",
    "print('yaw_init: ' + str(gen.tangentAngle[0]))\n",
    "print('Total Arc Length: ' + str(gen.arcLength[-1]/2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
