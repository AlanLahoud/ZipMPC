{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Function, Variable\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils import parameters_to_vector\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from mpc import mpc\n",
    "from mpc.mpc import GradMethods, QuadCost, LinDx\n",
    "#from mpc.dynamics import NNDynamics\n",
    "#import mpc.util as eutil\n",
    "from mpc.env_dx import frenet_kin_bicycle\n",
    "from mpc.track.src import simple_track_generator, track_functions\n",
    "\n",
    "import time\n",
    "import os\n",
    "import shutil\n",
    "import pickle as pkl\n",
    "import collections\n",
    "\n",
    "import argparse\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self, H):\n",
    "        super(NN, self).__init__()\n",
    "        self.fc1 = nn.Linear(H + 6, 128)  \n",
    "        self.fc2 = nn.Linear(128, 64)  \n",
    "        self.output1 = nn.Linear(64, 6) \n",
    "        self.output2 = nn.Linear(64, 6) \n",
    "\n",
    "    def forward(self, c, x0):\n",
    "        combined = torch.cat((c, x0), dim=1)\n",
    "        x = F.relu(self.fc1(combined))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        q = F.relu(self.output1(x)) + 0.3\n",
    "        p = self.output2(x)\n",
    "        return q, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try to create a track \n",
    "\n",
    "track_density = 300\n",
    "track_width = 0.5\n",
    "gen = simple_track_generator.trackGenerator(track_density,track_width)\n",
    "track_name = 'LONG_TRACK'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 0.3\n",
    "init = [0,0,0]\n",
    "\n",
    "track_function = {\n",
    "    'DEMO_TRACK'    : track_functions.demo_track,\n",
    "    'HARD_TRACK'    : track_functions.hard_track,\n",
    "    'LONG_TRACK'    : track_functions.long_track,\n",
    "    'LUCERNE_TRACK' : track_functions.lucerne_track,\n",
    "    'BERN_TRACK'    : track_functions.bern_track,\n",
    "    'INFINITY_TRACK': track_functions.infinity_track,\n",
    "    'SNAIL_TRACK'   : track_functions.snail_track\n",
    "}.get(track_name, track_functions.demo_track)\n",
    "    \n",
    "track_function(gen, t, init)\n",
    "    \n",
    "gen.populatePointsAndArcLength()\n",
    "gen.centerTrack()\n",
    "\n",
    "track_coord = torch.from_numpy(np.vstack([gen.xCoords, gen.yCoords, gen.arcLength, gen.tangentAngle, gen.curvature]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "\n",
    "def sample_xinit(n_batch):\n",
    "    def uniform(shape, low, high):\n",
    "        r = high-low\n",
    "        return torch.rand(shape)*r+low\n",
    "    sigma = uniform(n_batch, 0.01, 0.2)\n",
    "    d = uniform(n_batch, -0.1, 0.1)\n",
    "    phi = uniform(n_batch, -0.10*np.pi, 0.10*np.pi)\n",
    "    v = uniform(n_batch, 0., 0.5)\n",
    "    d_lb = softplus_op(-d-0.5*track_width)\n",
    "    d_ub = softplus_op(d-0.5*track_width)\n",
    "    xinit = torch.stack((sigma, d, phi, v, d_lb, d_ub), dim=1)\n",
    "    return xinit\n",
    "\n",
    "\n",
    "true_dx = frenet_kin_bicycle.FrenetKinBicycleDx(track_coord)\n",
    "mpc_T = 15\n",
    "n_batch = 8\n",
    "H_curve = 60\n",
    "use_NN_curv = False\n",
    "\n",
    "# Added here the bounds of U\n",
    "u_lower = torch.tensor([-2., -0.4]).unsqueeze(0).unsqueeze(0).repeat(mpc_T, n_batch, 1)\n",
    "u_upper = torch.tensor([2., 0.4]).unsqueeze(0).unsqueeze(0).repeat(mpc_T, n_batch, 1)\n",
    "u_init= torch.tensor([0.0, 0.0]).unsqueeze(0).unsqueeze(0).repeat(mpc_T, n_batch, 1)\n",
    "\n",
    "\n",
    "n_state = true_dx.n_state\n",
    "n_ctrl = true_dx.n_ctrl\n",
    "\n",
    "eps = 1e-2\n",
    "lqr_iter = 60\n",
    "grad_method = GradMethods.AUTO_DIFF\n",
    "\n",
    "softplus_op = torch.nn.Softplus(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nearest_index(point_f, ref_path):\n",
    "    return ((point_f[0] - ref_path[2,:])**2).argmin()\n",
    "    \n",
    "def compute_x_coord(point_f, ref_path, nearest_index):\n",
    "    return ref_path[0,nearest_index] - point_f[1]*torch.sin(ref_path[3,nearest_index])\n",
    "\n",
    "def compute_y_coord(point_f, ref_path, nearest_index):\n",
    "    return ref_path[1,nearest_index] + point_f[1]*torch.cos(ref_path[3,nearest_index])\n",
    "\n",
    "def frenet_to_cartesian(point_f, ref_path):     \n",
    "    nearest_index = get_nearest_index(point_f, ref_path)\n",
    "    x = compute_x_coord(point_f, ref_path, nearest_index)\n",
    "    y = compute_y_coord(point_f, ref_path, nearest_index)\n",
    "    \n",
    "    return torch.tensor([x, y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_progress(x_init, dx, _Q, _p, mpc_T=mpc_T):    \n",
    "        \n",
    "        pred_x, pred_u, pred_objs = mpc.MPC(\n",
    "            dx.n_state, dx.n_ctrl, mpc_T,\n",
    "            u_lower=u_lower, u_upper=u_upper, u_init=u_init,\n",
    "            lqr_iter=60,\n",
    "            verbose=0,\n",
    "            exit_unconverged=False,\n",
    "            detach_unconverged=True,\n",
    "            linesearch_decay=0.1,\n",
    "            max_linesearch_iter=10,\n",
    "            grad_method=grad_method,\n",
    "            eps=0.5,\n",
    "            n_batch=n_batch,\n",
    "        )(x_init, QuadCost(_Q, _p), dx)\n",
    "        \n",
    "        progress_loss = torch.mean(-pred_x[mpc_T-1,:,0] + pred_x[0,:,0])\n",
    "            \n",
    "        return progress_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_params = true_dx.params\n",
    "track_coord = track_coord.to(device)\n",
    "dx = true_dx.__class__(track_coord,env_params)\n",
    "\n",
    "q_penalty = .001*torch.ones(2).to(device)\n",
    "p_penalty = 1.*torch.ones(2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if use_NN_curv:\n",
    "    model = NN(H_curve)\n",
    "    opt = optim.Adam(model.parameters(), lr=5e-4)\n",
    "    q_penalty_batch = q_penalty.unsqueeze(0).repeat(n_batch,1)\n",
    "    p_penalty_batch = p_penalty.unsqueeze(0).repeat(n_batch,1)\n",
    "else:\n",
    "    true_q, true_p = true_dx.get_true_obj()\n",
    "    params1 = []\n",
    "    learn_q_logit_state = torch.ones(n_state-2, requires_grad=True).to(device)\n",
    "    learn_q_logit_input = torch.ones(n_ctrl, requires_grad=True).to(device)\n",
    "    learn_p_state = torch.zeros(n_state-2, requires_grad=True).to(device)\n",
    "    learn_p_input = torch.zeros(n_ctrl, requires_grad=True).to(device)\n",
    "    params1 += [learn_q_logit_state, learn_q_logit_input, learn_p_state, learn_p_input]\n",
    "    params = [{\n",
    "                'params': params1,\n",
    "                'lr': 2e-3,\n",
    "                'alpha': 0.99,\n",
    "                }]\n",
    "    opt = optim.RMSprop(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_curve_hor_from_x(x, track_coord, mpc_T):\n",
    "    idx_track_batch = ((x[:,0]-track_coord[[2],:].T)**2).argmin(0)\n",
    "    idcs_track_batch = idx_track_batch[:, None] + torch.arange(mpc_T)\n",
    "    curvs = track_coord[4,idcs_track_batch].float()\n",
    "    return curvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_to_batch_NN(q, p, n_batch, mpc_T):\n",
    "    Q_batch = torch.zeros(n_batch, q.shape[1], q.shape[1])\n",
    "    rows, cols = torch.arange(q.shape[1]), torch.arange(q.shape[1])  \n",
    "    Q_batch[:, rows, cols] = q \n",
    "    Q_batch = Q_batch.unsqueeze(0).repeat(\n",
    "                mpc_T, 1, 1, 1)    \n",
    "    p_batch = p.unsqueeze(0).repeat(mpc_T, 1, 1)\n",
    "    return Q_batch, p_batch\n",
    "\n",
    "def cost_to_batch(q, p, n_batch, mpc_T):\n",
    "    Q_batch = torch.diag(q).unsqueeze(0).unsqueeze(0).repeat(\n",
    "                mpc_T, n_batch, 1, 1\n",
    "            )\n",
    "    p_batch = p.unsqueeze(0).repeat(mpc_T, n_batch, 1)   \n",
    "    return Q_batch, p_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(500):\n",
    "    \n",
    "    x_init = sample_xinit(n_batch).to(device)\n",
    "    \n",
    "    if use_NN_curv:\n",
    "        curvs = get_curve_hor_from_x(x_init, track_coord, H_curve)\n",
    "        q_learned, p_learned = model(curvs, x_init)\n",
    "        q = torch.cat((q_learned, q_penalty_batch), dim=1)\n",
    "        p = torch.cat((p_learned, p_penalty_batch), dim=1)\n",
    "        q[:,1] = q[:,1] + 20.0\n",
    "        Q_batch, p_batch = cost_to_batch_NN(q, p, n_batch, mpc_T)\n",
    "    \n",
    "    else:\n",
    "        q = torch.cat((learn_q_logit_state,learn_q_logit_input,q_penalty),dim=0)\n",
    "        p = torch.cat((learn_p_state,learn_p_input,p_penalty), dim=0)  \n",
    "        q = q.clip(0.0001)\n",
    "        q[1] = q[1] + 20.0\n",
    "        Q_batch, p_batch = cost_to_batch(q, p, n_batch, mpc_T)\n",
    "        #print(q)   \n",
    "        \n",
    "    loss = get_loss_progress(x_init, dx, Q_batch, p_batch)\n",
    "\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    print('Batch:', i , ' Progression with MPC_T=',mpc_T ,': ', -round(loss.item(), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_test = 1\n",
    "mpc_T_test = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_init_test = sample_xinit(N_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_init_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below you can put any initial state you want (any that make sense)\n",
    "d = torch.tensor(0.0)\n",
    "d_lb = softplus_op(-d-0.5*track_width)\n",
    "d_ub = softplus_op(d-0.5*track_width)\n",
    "\n",
    "x_init_test = torch.tensor([[0.01, d, -.0, 0.01, d_lb, d_ub]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Added here the bounds of U\n",
    "u_lower_test = torch.tensor([-2., -.4]).unsqueeze(0).unsqueeze(0).repeat(mpc_T_test, N_test, 1)\n",
    "u_upper_test = torch.tensor([2., .4]).unsqueeze(0).unsqueeze(0).repeat(mpc_T_test, N_test, 1)\n",
    "u_init= torch.tensor([0.0, 0.0]).unsqueeze(0).unsqueeze(0).repeat(mpc_T_test, N_test, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_penalty.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N_sim_total = 100\n",
    "x_simulated = torch.zeros((N_sim_total,1,6))\n",
    "\n",
    "x_simulated[0] = x_init_test[0]\n",
    "\n",
    "q_penalty_batch = q_penalty.unsqueeze(0).repeat(N_test,1)\n",
    "p_penalty_batch = p_penalty.unsqueeze(0).repeat(N_test,1)\n",
    "\n",
    "for i in range(1,N_sim_total):\n",
    "      \n",
    "    \n",
    "    x_sim = x_simulated[i-1]\n",
    "    print(x_sim)\n",
    "        \n",
    "    if use_NN_curv:\n",
    "        curvs_test = get_curve_hor_from_x(x_sim, track_coord, H_curve)\n",
    "        q_test, p_test = model(curvs_test, x_sim)\n",
    "        q_test = torch.cat((q_test, q_penalty_batch), dim=1)\n",
    "        p_test = torch.cat((p_test, p_penalty_batch), dim=1)\n",
    "        q_test[:,1] = q_test[:,1] + 20.0\n",
    "        Q_test_batch, p_test_batch = cost_to_batch_NN(q_test, p_test, N_test, mpc_T_test) \n",
    "    \n",
    "    else:\n",
    "        q_test = torch.cat((learn_q_logit_state,learn_q_logit_input,q_penalty),dim=0)\n",
    "        p_test = torch.cat((learn_p_state,learn_p_input,p_penalty), dim=0) \n",
    "        q_test = q_test.clip(0.0001)\n",
    "        q_test[1] = q_test[1] + 100.0\n",
    "        Q_test_batch, p_test_batch = cost_to_batch(q_test, p_test, N_test, mpc_T_test)\n",
    "            \n",
    " \n",
    "    x_mpc_test, u_mpc_test, objs_mpc_test = mpc.MPC(\n",
    "                n_state, n_ctrl, mpc_T_test,\n",
    "                u_lower=u_lower_test, u_upper=u_upper_test, u_init=u_init,\n",
    "                lqr_iter=60,\n",
    "                verbose=0,\n",
    "                exit_unconverged=False,\n",
    "                detach_unconverged=True,\n",
    "                linesearch_decay=.4,\n",
    "                max_linesearch_iter=10,\n",
    "                grad_method=grad_method,\n",
    "                eps=.5,\n",
    "                n_batch=N_test,\n",
    "            )(x_sim, QuadCost(Q_test_batch, p_test_batch), dx)\n",
    "    #print(x_mpc_test[1])\n",
    "    x_simulated[i] = x_mpc_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_list = []\n",
    "y_list = []\n",
    "\n",
    "for i in range(x_simulated.shape[0]):\n",
    "    xy = frenet_to_cartesian(x_simulated[i,0,:2], track_coord)\n",
    "    x_list.append(xy[0].numpy())\n",
    "    y_list.append(xy[1].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_plot = np.array(x_list)\n",
    "y_plot = np.array(y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(10,5), dpi=150)\n",
    "gen.plotPoints(ax)\n",
    "#gen.pointAtArcLength(0)\n",
    "#gen.writePointsToYaml('../tracks/' + track_name + '.yaml', track_density)\n",
    "\n",
    "ax.scatter(x_plot, y_plot, s=4, color='red')\n",
    "#ax.scatter(0.1471, 0.8741, s=7, color='green')\n",
    "#ax.scatter(0.4500, 0.5100, s=100, color='blue')\n",
    "\n",
    "\n",
    "\n",
    "print('x_init: ' + str(gen.xCoords[0]))\n",
    "print('y_init: ' + str(gen.yCoords[0]))\n",
    "print('yaw_init: ' + str(gen.tangentAngle[0]))\n",
    "print('Total Arc Length: ' + str(gen.arcLength[-1]/2))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
